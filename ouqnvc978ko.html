<h2 data-label="152075" class="ltx_title_subsection">Training Deep Learning to Automate Image Labeling</h2><div>The deep learning model was trained to predict the XGBoosted labels that were based on citizen scientist ratings.&nbsp;A VGG16 neural network&nbsp;<cite class="ltx_cite raw v1">\cite{simonyan2014very}</cite>, pretrained on the ImageNet challenge dataset&nbsp;<cite class="ltx_cite raw v1">\cite{ILSVRC15}</cite> was used: we removed the top layer of the network, and then trained&nbsp; a final fully-connected layer followed by a single node output layer. The training of the final layer was run for 50 epochs and the best model on the validation set was saved. To estimate the variability of our the variability of the results of training, &nbsp;the model was trained through 10 different training  courses, each time with a different random initialization seed. Typically, training and validation loss scores were equal at around 10 epochs, after which the model usually began to overfit (training error decreased, while validation error increased). In each of the 10 training courses, we used the model with the lowest validation error &nbsp;for inference on the held out test set, and calculated the ROC AUC. AUC may be a problematic statistic when the test-set is imbalanced <cite class="ltx_cite raw v1">\cite{saito2015precision}</cite>, but in this case, the test-set is almost perfectly balanced (see Methods). We found that a deep learning network trained on citizen scientist generated labels matched closer to expert ratings than citizen scientist generated labels alone: the deep learning model had an &nbsp;AUC of 0.99 (+/- standard deviation of 0.12).</div><div></div><div><b>Citizen scientists accurately scale up expert ratings but, ideally, we would have a fully automated approach that can be applied to new data as it becomes available. Thus, we trained a deep learning model to predict the XGBoosted labels that were based on aggregated citizen scientist ratings.&nbsp;A VGG16 neural network&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{simonyan2014very}</cite> pretrained on the ImageNet challenge dataset &nbsp;<cite class="ltx_cite raw v1">\cite{ILSVRC15}</cite> was used: we removed the top layer of the network, and then trained&nbsp; a final fully-connected layer followed by a single node output layer. The training of the final layer was run for 50 epochs and the best model on the validation set was saved. To estimate the variability of &nbsp;training,  the model was separately trained through 10 different training courses, each time with a different random initialization seed. Typically, training and validation loss scores were equal at around 10 epochs, after which the model usually began to overfit (training error decreased, while validation error increased). In each of the 10 training courses, we used the model with the lowest validation error  for inference on the held out test set, and calculated the ROC AUC. AUC may be a problematic statistic when the test-set is imbalanced&nbsp;<cite class="ltx_cite raw v1">\cite{saito2015precision}</cite>, but in this case, the test-set is almost perfectly balanced (see Methods&nbsp;). Thus, we found that a deep learning network trained on citizen scientist generated labels was a better match to expert ratings than citizen scientist generated labels alone: the deep learning model had an &nbsp;AUC of 0.99 (+/- standard deviation of 0.12).&nbsp;</b></div>