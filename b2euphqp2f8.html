<h2 data-label="188132" class="ltx_title_subsection">Rating aggregation with XGBoost</h2><div>The distribution on Figure&nbsp;<span class="au-ref raw v1">\ref{358654}</span> A was bimodal, as expected, but the left peak (corresponding to failed slices) was shifted to the right. Because these were gold standard images, selected because neuroimaging experts who confidently passed or failed these images, we expected the left peak of the bimodal distribution to be at 0. This implied that some users were incorrectly passing images. To select the users who rated more similarly to the gold standard raters, we trained an XGBoost classifier&nbsp;<cite class="ltx_cite raw v1">\cite{chen2016xgboost}</cite> implemented in Python (<a href="http://xgboost.readthedocs.io/en/latest/python/python_intro.html">http://xgboost.readthedocs.io/en/latest/python/python_intro.html</a>) using the cross-validation functions from the scikit-learn Python library&nbsp;<cite class="ltx_cite raw v1">\cite{pedregosa2011scikit}</cite>. We used 600 estimators, and grid searched over a&nbsp; stratified 10-fold cross-validation within the training set to select the optimal maximum depth (2 vs 6) and learning rate (0.01, 0.1).  The features of the model were the braindr players and their average rating on each slice (i.e. an observation). We trained the classifier on splits of various sizes of the data to test the dependence on training size (see Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>A). We used the model trained with n=670 to extract the probability scores of the classifier on all 3609 slices in braindr (see Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>B). The distribution of probability scores in Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>B better matches our expectations of the data; a bimodal distribution with peaks at 0 and 1. Feature importances were extracted from the model and plotted in Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>C, and plotted against total number of "gold" standard image ratings in Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>D.</div><div></div><div><b>To aggregate citizen scientist ratings, we weighted citizen scientists based on how consistent their ratings were with the gold standard. We trained an XGBoost classifier&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{chen2016xgboost}</cite> implemented in Python (</b><a href="http://xgboost.readthedocs.io/en/latest/python/python_intro.html"><b>http://xgboost.readthedocs.io/en/latest/python/python_intro.html</b></a><b>) using the cross-validation functions from the scikit-learn Python library&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{pedregosa2011scikit}</cite>. We used 600 estimators, and grid searched over a&nbsp; stratified 10-fold cross-validation within the training set to select the optimal maximum depth (2 vs 6) and learning rate (0.01, 0.1). The features of the model were the citizen scientists &nbsp;and each observation was a slice, with the entries in the design matrix set to be the average rating of a specific citizen scientist on a particular slice. We trained the classifier on splits of various sizes of the data to test the dependence on training size (see Figure&nbsp;<span class="au-ref raw v1">\ref{468392}</span>A). We used the model trained with n=670 to extract the probability scores of the classifier on all 3609 slices in braindr (see Figure&nbsp;??B). While equally weighting each citizen scientistâ€™s ratings results in a bimodal distribution with a lower peak that is shifted up from zero (Figure ???A), the distribution of probability scores in Figure&nbsp;???B more accurately matches our expectations of the data; a bimodal distribution with peaks at 0 and 1. Feature importances were extracted from the model and plotted in Figure&nbsp;???C, and plotted against total number of gold standard image ratings in Figure&nbsp;???D.</b></div>