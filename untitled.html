<div></div><h1 data-label="712766" class="ltx_title_section">Introduction</h1><div>Many research fields ranging from astronomy, to genomics, to neuroscience are entering an era of “Big Data”. Large and complex datasets promise to address many scientific questions, but they also present a new set of challenges. For example, over the last few years human neuroscience&nbsp;has evolved into a “Big Data” field. In the past, individual groups would each collect their own relatively small samples of data from a limited group individuals. More recently, large data sets collected from many thousands of individuals are increasingly more common. This transition has been facilitated through &nbsp;assembly of large aggregated datasets, containing measurements from many individuals, and collected through consortium efforts such as the Human Connectome Project. These efforts, and the large datasets that they are assembling promise to enhance our understanding&nbsp; of the relationship between brain anatomy, brain activity and cognition. The field is experiencing a “paradigm shift”&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Fan_2014}</cite>, where our once established scientific procedures are morphing as dictated by the new challenges posed by large datasets.&nbsp;We’ve seen a shift from desktop computers to cyberinfrastructure&nbsp;<cite class="ltx_cite raw v1">\cite{Van_Horn_2013}</cite>, from small studies siloed in individual labs to an explosion of data sharing initiatives&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Ferguson_2014,Poldrack_2014}</cite>, and an overall shift in statistical thinking and computational methods&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Fan_2014}</cite> that can accommodate large datasets.&nbsp;But one often overlooked aspect of our protocols in neuroimaging has not yet evolved to the needs of Big Data: expert decision making.&nbsp;</div><div></div><div>Specifically, decisions made by scientists with expertise in neuroanatomy and MRI methods (i.e., neuroimaging experts) through visual inspection of imaging data cannot be accurately scaled to large datasets.&nbsp;These are decisions that generally rely on extensive domain expertise like tracing &nbsp;anatomical regions of interest (ROIs), editing fascicle models from streamline tractography &nbsp;<cite class="ltx_cite raw v1">\cite{Jordan_2017}</cite>, evaluating cross-modality image alignment, and quality control of images at each stage of processing.&nbsp;On large datasets, especially longitudinal multisite consortium studies, these expert decisions cannot be reliably replicated because the timeframe of these studies is long, individual experts get fatigued, and training teams of experts is time consuming, difficult and costly. As datasets grow to hundreds of thousands of brains it is no longer feasible to depend on manual interventions.</div><div></div><div>One solution to this problem is to train machines to emulate experts’ decisions. However, there are many cases in which even where automated algorithms exist, expert decision-making is still required for optimal results. For example, a variety of image segmentation algorithms have been developed to replace manual ROI editing, with  Freesurfer&nbsp;<cite class="ltx_cite raw v1">\cite{fischl2012freesurfer}</cite>, FSL&nbsp;<cite class="ltx_cite raw v1">\cite{Patenaude_2011}</cite>, ANTS&nbsp;<cite class="ltx_cite raw v1">\cite{Avants_2011}</cite>, and SPM&nbsp;<cite class="ltx_cite raw v1">\cite{Ashburner_2005}</cite>  all offering automated segmentation tools for standard structures. But these algorithms were developed on a specific type of image (T1-weighted) and on a specific type of brain (those of healthy controls). Pathological brains, or those of children or the elderly may violate the assumptions of these algorithms, and their outputs often still require manual expert editing. Similarly, in tractography, a set of anatomical ROIs can be used to target or constrain streamlines to automatically extract fascicles of interest&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{CATANI_2008}</cite>.&nbsp;But again, abnormal brain morphometry resulting from pathology would still require expert editing&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Jordan_2017a}</cite>.&nbsp; The delineation of retinotopic maps in visual cortex is another task that has been recently automated&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Benson2014,Benson2012}</cite>, but these procedures are limited to only a few of the known retinotopic maps and substantial expertise is still required to delineate the other known maps&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Winawer2017,Wandell2011}</cite>. Finally, one of the first steps of image processing pipelines is quality assurance/quality control. There are several automated methods to quantify image quality, based on MRI physics and the statistical properties of images, and these have been collected under one umbrella in an algorithm called MRIQC&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Esteban2017}</cite>. However, these methods are specific to T1-weighted images, and cannot generalize to different image acquisition methods. To address all of these cases, and scale to new, unforeseen challenges, we need a general-purpose framework that can train machines to emulate experts for any purpose.&nbsp;</div><div></div><div>Deep learning via convolutional neural networks (CNNs) has shown promise in a variety of biomedical image processing tasks. Modeled loosely on the human visual system, CNNs can be trained for a variety of image classification and segmentation tasks using the same architecture. For example, the U-Net&nbsp;<cite class="ltx_cite raw v1">\cite{ronneberger2015u}</cite>&nbsp;which was originally built for segmentation of neurons in electron microscope images, has also been adapted to segment macular edema in optical coherence tomography images&nbsp;<cite class="ltx_cite raw v1">\cite{Lee_2017}</cite>, to segment breast and fibroglandular tissue&nbsp;<cite class="ltx_cite raw v1">\cite{Dalm__2017}</cite>, and a 3D adaptation was developed to segment the&nbsp;<i>Xenopu</i>s kidney&nbsp;<cite class="ltx_cite raw v1">\cite{cciccek20163d}</cite>.&nbsp;</div><div></div><div><b>Deep learning via convolutional neural networks (CNNs) has shown promise in a variety of biomedical image processing tasks. Modeled loosely on the human visual system, CNNs can be trained for a variety of image classification and segmentation tasks using the same architecture. For example, the U-Net&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{ronneberger2015u}</cite>&nbsp;which was originally built for segmentation of neurons in electron microscope images, has also been adapted to segment macular edema in optical coherence tomography images&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Lee_2017}</cite>, to segment breast and fibroglandular tissue&nbsp;&nbsp;<cite class="ltx_cite raw v1">\cite{Dalm__2017}</cite>, and a 3D adaptation was developed to segment the&nbsp;Xenopus kidney&nbsp;<cite class="ltx_cite raw v1">\cite{cciccek20163d}</cite>.&nbsp;</b></div>